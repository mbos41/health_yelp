load("~/Documents/INLS_613/yelp_dataset/reviews_fltr.Rda")
WI_rest <- reviews_fltr %>%
filter(state == 'WI') %>%
filter(str_detect(categories, "Restaurants")) %>%
filter(review_count > 20) %>%
mutate(year = str_sub(date, 1, 4))
base_data <- WI_rest %>%
filter(year > 2009 & year < 2016) %>%
mutate(price = `attributes.Price Range`) %>%
select(-`attributes.Price Range`)
test_data <- WI_rest %>%
filter(year == 2016)
# Leaves 6,292 reviews in 2016 to use for training
user_stats <- base_data %>%
group_by(user_id) %>%
summarise(user_avg = mean(stars),
user_sd = sd(stars),
user_count = n()
)
base_data <- base_data %>% inner_join(user_stats, by = "user_id")
test_data <- test_data %>% inner_join(user_stats, by = "user_id")
# Normalize star ratings by user avg rating and standard deviation
# Remove user's with standard deviation = 0 (users with only 1 review, or no variation in reviews)
norm_data <- base_data %>%
mutate(norm_stars = ((stars - user_avg) / user_sd)) %>%
filter(user_sd > 0) %>%
mutate(positive = (norm_stars >= 0))
user_price_stats <- norm_data %>%
group_by(user_id, price) %>%
summarise(price_rating = mean(positive))
price_t <- dcast(user_price_stats, user_id~price, fun.aggregate=mean)[,1:5]
names(price_t) <-  c("user_id", "Price1", "Price2", "Price3", "Price4")
price_t[is.na(price_t)] <-  0
rest_stats <- norm_data %>%
group_by(business_id) %>%
summarise(rest_count = n())
test_data <- test_data %>% inner_join(rest_stats, by = "business_id")
test_norm_data <- test_data %>%
mutate(norm_stars = ((stars - user_avg) / user_sd)) %>%
filter(user_sd > 0) %>%
mutate(positive = (norm_stars >= 0))
# Aggregate review text by restaurant id
review_text <- norm_data %>% select(review_id, text, stars) %>% filter(nchar(text) > 30)
stop_words <- stopwords("SMART")
#stop_words <- c("a", "the", "is", "and", "but", "or", "on", "it", "in", "i'm", "i", "of", "you", "your", "at", "an", "to", "was", "for", "with", "that", "my", "we", "this",
#                "they", "had", "have", "were", "are", "so", "be", "as", "place", "im")
# pre-processing:
reviews <- gsub("'", "", review_text$text)  # remove apostrophes
reviews <- gsub("[[:punct:]]", " ", reviews)  # replace punctuation with space
reviews <- gsub("[[:cntrl:]]", " ", reviews)  # replace control characters with space
reviews <- gsub("^[[:space:]]+", "", reviews) # remove whitespace at beginning of documents
reviews <- gsub("[[:space:]]+$", "", reviews) # remove whitespace at end of documents
reviews <- tolower(reviews)  # force to lowercase
# tokenize on space and output as a list:
doc.list <- strsplit(reviews, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
set.seed(200)
k = 5
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
sldaOut <- slda.em(documents = documents, vocab =  vocab, K = k, num.e.iterations = 10, num.m.iterations = 10, variance = 0.25,
alpha = 0.1, eta = 0.1, annotations = review_text$stars, params, lambda = 1.0, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words <- top.topic.words(sldaOut$topics, num.words = 20)
write.table(top_words, "~/Documents/INLS_613/top_words2.txt", sep="\t")
topic_coeff <- as.data.frame(sldaOut$coef)
doc_sums <-  as.data.frame(t(sldaOut$document_sums)) %>%
mutate(total_words = (V1+V2+V3+V4+V5))
topic_correlations <- cor(doc_sums)
doc_sums$review_id <-  review_text$review_id
doc_sums$text <- review_text$text
#save(doc_sums, file = "~/Documents/INLS_613/yelp_dataset/doc_sums.Rda")
doc_sum <- load("~/Documents/INLS_613/yelp_dataset/doc_sums.Rda")
sim_prep <-  norm_data %>% left_join(doc_sums, by = "review_id")
sim_matrix_p <- sim_prep %>%
filter(positive == TRUE) %>%
group_by(user_id) %>%
summarise(tot_words = sum(total_words),
P1 = sum(V1)/tot_words, P2 = sum(V2)/tot_words, P3 = sum(V3)/tot_words, P4 = sum(V4)/tot_words, P5 = sum(V5)/tot_words) %>%
select(-tot_words)
sim_matrix_n <- sim_prep %>%
filter(positive == FALSE) %>%
group_by(user_id) %>%
summarise(tot_words = sum(total_words),
N1 = sum(V1)/tot_words, N2 = sum(V2)/tot_words, N3 = sum(V3)/tot_words, N4 = sum(V4)/tot_words, N5 = sum(V5)/tot_words) %>%
select(-tot_words)
sim_matrix <- sim_matrix_n %>% inner_join(sim_matrix_p, by = "user_id")
sim_matrix <- sim_matrix %>% inner_join(price_t, by = "user_id")
sim_matrix[is.na(sim_matrix)] <-  0
write.table(sim_matrix, "~/Documents/INLS_613/sim_matrix2.txt", sep="\t")
filter_kNN <- function(user, business, k){
# Find other users who have rated this restaurant
filtered <- norm_data %>%
filter(business_id == business) %>%
select(user_id, norm_stars)
# Add active user to list
filtered2 <- rbind(filtered, c(user, "NA"))
# Filter similarity matrix to users who have rated this restaurant
sim_filter <- sim_matrix %>% inner_join(filtered2, by = "user_id") %>% select(-norm_stars)
# Transpose data frame and move active user to 2nd column
sim_t <- dcast(melt(sim_filter), ...~user_id, fun.aggregate=mean)
sim_t <- sim_t %>% select(variable, get(user), everything())
dist <-  NULL
user_id <-  NULL
for(i in 1:(ncol(sim_t)-2)) {
dist[i] <- as.numeric(cor(sim_t[,2], sim_t[,i+2]))
user_id[i] <- as.character(names(sim_t)[i+2])
}
# Sort k nearest neighbors
#sorted <- arrange(as.data.frame(cbind(dist, user_id)), desc(dist))[1:k,] %>% mutate(weight = 1 + (1 - row_number())*(1/k))
sorted <- arrange(as.data.frame(cbind(dist, user_id)), desc(dist)) %>% filter(!is.na(dist))
# Weight neighbors from 1 to 0.2
if (length(sorted$dist) == 0) {
avg <- filtered %>% filter(user_id != user)  %>%
summarise(predict = mean(as.numeric(norm_stars)))
avg %>% mutate(user_id = user, business_id = business)
} else {
#Assign active user the average rating of K-NN
avg <- filtered %>% filter(user_id != user) %>%
inner_join(sorted, by ="user_id")  %>%
summarise(predict = sum(as.numeric(dist)*as.numeric(norm_stars))/sum(as.numeric(dist)))
avg %>% mutate(user_id = user, business_id = business)
#avg <- filtered %>% filter(user_id != user) %>%
#  inner_join(sorted, by ="user_id")  %>%
#  summarise(predict = sum(as.numeric(weight)*as.numeric(norm_stars))/sum(as.numeric(weight)))
#avg %>% mutate(user_id = user, business_id = business)
}
}
# For testing
business <- "EH7sT3dV-B365RETWCQhIA"
user <- "nmTG128TupL0-tnhe1h7CQ"
out <- filter_kNN(user, business, 1)
set.seed(203)
# Draw balanced sample to test predictions
positive_sample <- sample(which(test_norm_data$positive == "TRUE"), size = 750, replace = F)
negative_sample <- sample(which(test_norm_data$positive == "FALSE"), size = 750, replace = F)
test <- test_norm_data[c(positive_sample, negative_sample),]
test <- test %>%
select(user_id, business_id, stars, norm_stars, user_avg, user_sd, positive)
predictions  <-  data.frame(user_id = character(), business_id = character(), predict = numeric())
predictions2  <-  data.frame(user_id = character(), business_id = character(), predict = numeric())
# Turn off warnings temporarily
oldw <- getOption("warn")
options(warn = -1)
# Run kNN and save in predictions dataframe
for (i in 1:length(test$user_id)) {
predictions <- rbind(predictions, filter_kNN(as.character(test[i,1]), as.character(test[i,2]), 5))
}
check <- test
check$predict <- predictions$predict
# Calculate accuracy statistics
check <- check %>%
mutate(correct_norm = (positive == (predict > 0)),
predict_unnorm = (predict*user_sd + user_avg),
stars_bin = (stars >= 4),
correct_4 = (stars_bin == (predict_unnorm >= 4)),
sq_diff = ((stars - predict_unnorm)^2),
diff = (stars - predict_unnorm),
abs_diff = ((abs(stars - predict_unnorm))),
user_predict = ((user_avg >= 4) == stars_bin),
user_sq_diff = (user_avg - stars)^2,
user_abs_diff = (abs((user_avg - stars)))
)
# All data in training dataset has been included in the topic models, so test data will be much better measure of success
# Test data can be 2016 reviews from users and restaurants in the training set
(baseline_norm <- sum(check$positive) / length(check$correct_norm))
(accuracy_norm <- sum(check$correct_norm) / length(check$correct_norm))
(baseline_norm <- sum(check2$positive) / length(check2$correct_norm))
(accuracy_norm <- sum(check2$correct_norm) / length(check2$correct_norm))
#(accuracy_4 <- sum(check$correct_4) / length(check$correct_4))
#(baseline_4 <- sum(check$stars_bin) / length(check$correct_norm))
(positive_acc <- sum(check$correct_norm & check$positive) / sum(check$positive == TRUE))
(negative_acc <- sum(check$correct_norm & check$positive == FALSE) / sum(check$positive == FALSE))
(mean(check2$sq_diff))
(mean(check$user_sq_diff))
(mean(check2$abs_diff))
(mean(check$user_abs_diff))
check_analysis <- check %>% inner_join(user_stats, by = "user_id")
check_analysis$cnt_bins <- ceiling(check_analysis$user_count / 5)
ggplot(data = check_analysis) + geom_bar(mapping = aes(x = factor(stars),  fill = correct_norm), position = "fill") +
ylab("Accuracy") + xlab("Actual Rating") + guides(fill = guide_legend(title = "Prediction"))
ggplot(data = check_analysis) + geom_bar(mapping = aes(x = factor(stars))) +
ylab("Count") + xlab("Actual Rating")
ggplot(data = check_analysis) + geom_point(mapping = aes(x = diff, y= stars, alpha = .05, color = correct_norm)) +
ylab("Actual Rating") + xlab("Actual - Predicted ")
ggplot(data = check_analysis, aes(user_count, fill = correct_norm, position = "fill")) +
geom_histogram(breaks=seq(0, 125, by =5)) + ylab("Count") + xlab("Prior User Reviews") +
guides(fill = guide_legend(title = "Prediction"))
ggplot(data = check_analysis) + geom_bar(mapping = aes(x = factor(cnt_bins),  fill = correct_norm), position = "fill")
ggplot(data = check_analysis) + geom_histogram(mapping = aes(x = diff)) + ylab("Count") + xlab("Actual - Predicted")
check_ttest <- check %>% select(sq_diff)
check_ttest$sq_diff_baseline <- check2$sq_diff
t.test(check_ttest$sq_diff, check_ttest$sq_diff_baseline, paired=TRUE)
p <- (0.6153333*1500 + 0.6273333*1500)/3000
SE <- ((p*(1-p))*(1/1500 + 1/1500))^0.5
z <- (0.6153333 - 0.6273333) / SE
(p_value <- dnorm(z))
options(warn = oldw)
View(norm_data)
norm_data
norm_data  %>% group_by(user_id)  %>% count()
25330 / 4003
# Aggregate review text by restaurant id
reviews_by_rest <- aggregate(text ~ business_id, paste, data = norm_data)
# Clean up text for creating restaurant document term matrix
docs_rest <- Corpus(VectorSource(reviews_by_rest$text))
docs_rest <-tm_map(docs_rest,content_transformer(tolower))
#remove punctuation
docs_rest <- tm_map(docs_rest, removePunctuation)
#Strip digits
docs_rest <- tm_map(docs_rest, removeNumbers)
#remove stopwords
docs_rest <- tm_map(docs_rest, removeWords, stopwords("english"))
#remove whitespace
docs_rest <- tm_map(docs_rest, stripWhitespace)
#Create document-term matrix (Limit to terms appearing in at least 10 restaurants and no more than 200 restaurants)
dtm_r <- DocumentTermMatrix(docs_rest, control=list(wordLengths=c(3, 20),
bounds = list(global = c(10,200))))
dtm_merge <- as.data.frame(as.matrix(dtm_r))
dtm_merge$business_id <- reviews_by_rest$business_id
#Set parameters for Gibbs sampling
burnin <- 200
iter <- 300
thin <- 300
seed <-list(2003)
nstart <- 1
best <- TRUE
#Number of topics
k <- 20
ldaOut <-LDA(dtm_r, k, method='Gibbs', control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
library(topicmodels)
ldaOut <-LDA(dtm_r, k, method='Gibbs', control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
ldaOut.topics_r <- as.data.frame(as.matrix(topics(ldaOut)))
ldaOut.terms_r <- as.matrix(terms(ldaOut,10))
View(ldaOut.terms_r)
write.table(ldaOut.terms_r, "~/Documents/INLS_613/top_words20.txt", sep="\t")
install.packages("itsmr")
library(itsmr)
1+0+1+4+9+16
31*(1/6)
16**2
1+0+1+16+81+256
355/6 - (31/6)**2
16/9
library(jsonlite)
review_data <- stream_in(file("~/Documents/COMP_790/yelp_boston_academic_dataset/yelp_academic_dataset_review.json"))
review_flat <- flatten(review_data)
review_tbl <- as_data_frame(review_flat)
View(review_flat)
x  <- c(2, 1.4, 4, 3, 2.2)
x
sd(x)
dif  <- c(.6**2, 1.2**2, 1.4**2, .4**2)
sum(dif)
sum(dif)/4
sum(dif)/3
(sum(dif)/4)**.5
?sd
mean(dif)
mean(x)
sd(x)
set.seed(1)
series1 <- specify(0,0,4)
library(itsmr)
series1 <- specify(0,0,4)
sim(series1, 200)
plot(series1)
plota(series1)
series2 <- specify(0,0.5,3)
sim2 <- sim(series2, 200)
plota(series2)
.5/1.25
(1/16)*(4*5.26 + 2*3*4.74 + 2*2*4.26 + 2*3.84)
(1/16)*(4*5.26 + 2*3*-4.74 + 2*2*4.26 + 2*-3.84)
library(itsmr)
set.seed(1)
series1 <- specify(0,0,4)
sim1 <- sim(series1, 200)
plot(sim1)
plota(sim1)
plot.ts(sim1)
getwd()
knitr::opts_chunk$set(echo = TRUE)
deaths <- scan("deaths.txt")
library(itsmr)
plotc(deaths)
acf(deaths,lag.max=40,type=“correlation”,plot=TRUE)
acf(deaths,lag.max=40,type="correlation",plot=TRUE)
s <- season(deaths,12)
s
?season
plotc(deaths, s)
e <- deaths - s
plotc(e)
acf(e,lag.max=40,type="correlation",plot=TRUE)
?Resid
?trend
s <- season(deaths, 12)
y <- deaths - s
m <- trend(y, 2)
e <- deaths - s - m
plotc(e)
acf(e,lag.max=40,type="correlation",plot=TRUE)
e2 <- deaths - s - m
test(e2)
xv=c("season",12)
a = arma(e2, 0, 0)
forecast(deaths,xv,a,h=24,opt=2)
?arma
a
?forecast
4 + 1.5^2 + .25*4 + 4 + 1.5^2*4 + .25*4
1 + 1.5^2 + 0.25 + 1 + 1.5^2 + 0.25
-1.5 - 1.5 - 0.75 -0.75 -1.5 - 1.5 -0.75 -0.75
(1.5 + 0.75 + 1.5 + 0.75)
4.5*4
1.5^2
-1 - 2.25 - 0.25 - 1 -2.25 - 0.25
4*4.5
knitr::opts_chunk$set(echo = TRUE)
library(itmsr)
library(itsmr)
a=specify(ar=c(0.5),ma=c(0.5),sigma2=1)
?specify
a
a$phi
ar.inf(a,n=5)
?ar.inf
ma.inf(a,n=5)
?pnorm
pnorm(112, 115, (10/(20^0.5)))
10/(20^0.5)
-3/2.236
20^0.5
pnorm(-1.34)
pnorm(1.34)
?qt
qt(.05,24)
qt(.95,24)
5*1.71
6/1.71
3.51*5
1.96*8.97
1.65*.897
20^0.5
-3/(10/4.47)
qt(.05,24)
6/1.71
3.5*5
install.packages("glmnet")
library(lda)
library(tidyverse)
library(stringr)
library(tm)
library(glmnet)
setwd("Documents/COMP_790/health_yelp")
raw <- read.csv('data/analysis_data.csv')
raw$pos_text_temp <- trimws(as.character(raw$pos_text))
raw$neg_text_temp <- trimws(as.character(raw$neg_text))
analysis <- raw %>%
mutate(year = str_sub(date, 1, 4),
wgt_viol = (X.*1 + X..*2 + X...*5),
pos_text = ifelse(nchar(pos_text_temp) == 0, 'blank', pos_text_temp),
neg_text = ifelse(nchar(neg_text_temp) == 0, 'blank', neg_text_temp))
View(analysis)
analysis %>% group_by(year) %>% summarise(count = n())
train <- analysis %>%
filter(year < 2010)
train$mean_err = (train$wgt_viol - 10.44)^2
summary(train$mean_err)
?mean
summary(train$wgt_viol)
mean_baseline_err = mean((train$wgt_viol - 10.44)^2)^0.5
docs <- Corpus(VectorSource(train$pos_text))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
dtm <- DocumentTermMatrix(docs, control=list(wordLengths=c(3, 20),
bounds = list(global = c(10,200))))
dtm_df <- as.data.frame(as.matrix(dtm))
View(dtm_df)
dim(dtm)
summary(col_sums(dtm))
summary(colSums(dtm))
summary(colSums(as.matrix(dtm))
# pre-processing:
pos_text <- gsub("'", "", train$pos_text)  # remove apostrophes
pos_text <- gsub("[[:punct:]]", " ", pos_text)  # replace punctuation with space
pos_text <- gsub("[[:cntrl:]]", " ", pos_text)  # replace control characters with space
pos_text <- gsub("^[[:space:]]+", "", pos_text) # remove whitespace at beginning of documents
pos_text <- gsub("[[:space:]]+$", "", pos_text) # remove whitespace at end of documents
pos_text <- tolower(pos_text)  # force to lowercase
######################################################################
###############       Supervised LDA approach          ###############
######################################################################
stop_words <- stopwords("SMART")
# tokenize on space and output as a list:
doc.list <- strsplit(pos_text, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 50
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
set.seed(200)
k = 10
params <- sample(c(0, 0), k, replace=TRUE)
t1 <- Sys.time()
sldaOut <- slda.em(documents = documents, vocab =  vocab, K = k, num.e.iterations = 50, num.m.iterations = 100, variance = 125,
alpha = 1, eta = 1, annotations = train$wgt_viol, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
# 50/100 iterations seems to show only small changes from 20/20 iterations
top_words2 <- top.topic.words(sldaOut$topics, num.words = 20)
topic_coeff2 <- as.data.frame(sldaOut$coef)
pred2 <- slda.predict(documents = documents, topics = sldaOut$topics, model = sldaOut$model, alpha = 1, eta = 1)
train$predicted2 <-  pred2
rmse <- mean((train$wgt_viol - train$pred2)^2)^0.5
summary(colSums(as.matrix(dtm)))
library(slam)
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm_filter <- dtm[,term_tfidf >= 0.1]
dtm_filter <- dtm[row_sums(dtm) > 0,]
dtm_filter <- dtm[,term_tfidf >= 0.1]
dtm_filter <- dtm[,term_tfidf >= 0.15]
dtm <- DocumentTermMatrix(docs, control=list(wordLengths=c(3, 20)))
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm_filter <- dtm[,term_tfidf >= 0.05]
dtm_filter <- dtm[,term_tfidf >= 0.1]
dtm_merge <- as.data.frame(as.matrix(dtm_filter))
View(dtm_merge)
View(dtm_merge)
View(train)
ggplot(data = train) + geom_histogram(mapping = aes(x = log(wgt_viol + 1)))
dtm_merge$wgt_viol_log <- log(train$wgt_viol + 1)
View(dtm_merge)
dtm_merge <- as.data.frame(as.matrix(dtm_filter))
train <- analysis %>%
filter(year < 2010) %>%
log_wgt_viol = log(wgt_viol + 1)
train <- analysis %>%
filter(year < 2010) %>%
log_wgt_viol = log(wgt_viol + 1)
train <- analysis %>%
filter(year < 2010) %>%
log_wgt_viol = log(wgt_viol + 1)
train <- analysis %>%
filter(year < 2010) %>%
mutate(log_wgt_viol = log(wgt_viol + 1))
View(train)
ggplot(data = train) + geom_histogram(mapping = aes(x = wgt_viol))
summary(train$log_wgt_viol)
log_mean_baseline_err = mean((train$log_wgt_viol - 1.97)^2)^0.5
fit <- glmnet(dtm_merge, train$log_wgt_viol)
View(dtm_merge)
?glmnet
dtm_merge <- as.matrix(dtm_filter)
fit <- glmnet(dtm_merge, train$log_wgt_viol)
print(fit)
plot(fit)
cvfit <- cv.glmnet(dtm_merge, train$log_wgt_viol)
plot(cvfit)
cvfit$lambda.min
?log
exp(.019)
coeffs <- coef(cvfit, s = "lambda.min")
coeffs <- as.matrix(coef(cvfit, s = "lambda.min"))
coeffs[1:10]
(coeffs <- as.matrix(coef(cvfit, s = "lambda.min")))
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
v <- sort(colSums(m), decreasing=TRUE)
head(v, 10)
head(v, 30)
summary(col_sums(dtm))
v[100:125]
v[1000:1025]
v[3000:3025]
v[5000:5025]
term_cnts <- col_sum(dtm)
term_cnts <- col_sums(dtm)
dtm_filter <- dtm[, term_cnts > 50]
dtm_merge <- as.matrix(dtm_filter)
cvfit <- cv.glmnet(dtm_merge, train$log_wgt_viol)
