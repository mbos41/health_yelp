doc_sum <- load("~/Documents/INLS_613/yelp_dataset/doc_sums.Rda")
sim_prep <-  norm_data %>% left_join(doc_sums, by = "review_id")
sim_matrix_p <- sim_prep %>%
filter(positive == TRUE) %>%
group_by(user_id) %>%
summarise(tot_words = sum(total_words),
P1 = sum(V1)/tot_words, P2 = sum(V2)/tot_words, P3 = sum(V3)/tot_words, P4 = sum(V4)/tot_words, P5 = sum(V5)/tot_words) %>%
select(-tot_words)
sim_matrix_n <- sim_prep %>%
filter(positive == FALSE) %>%
group_by(user_id) %>%
summarise(tot_words = sum(total_words),
N1 = sum(V1)/tot_words, N2 = sum(V2)/tot_words, N3 = sum(V3)/tot_words, N4 = sum(V4)/tot_words, N5 = sum(V5)/tot_words) %>%
select(-tot_words)
sim_matrix <- sim_matrix_n %>% inner_join(sim_matrix_p, by = "user_id")
sim_matrix <- sim_matrix %>% inner_join(price_t, by = "user_id")
sim_matrix[is.na(sim_matrix)] <-  0
write.table(sim_matrix, "~/Documents/INLS_613/sim_matrix2.txt", sep="\t")
filter_kNN <- function(user, business, k){
# Find other users who have rated this restaurant
filtered <- norm_data %>%
filter(business_id == business) %>%
select(user_id, norm_stars)
# Add active user to list
filtered2 <- rbind(filtered, c(user, "NA"))
# Filter similarity matrix to users who have rated this restaurant
sim_filter <- sim_matrix %>% inner_join(filtered2, by = "user_id") %>% select(-norm_stars)
# Transpose data frame and move active user to 2nd column
sim_t <- dcast(melt(sim_filter), ...~user_id, fun.aggregate=mean)
sim_t <- sim_t %>% select(variable, get(user), everything())
dist <-  NULL
user_id <-  NULL
for(i in 1:(ncol(sim_t)-2)) {
dist[i] <- as.numeric(cor(sim_t[,2], sim_t[,i+2]))
user_id[i] <- as.character(names(sim_t)[i+2])
}
# Sort k nearest neighbors
#sorted <- arrange(as.data.frame(cbind(dist, user_id)), desc(dist))[1:k,] %>% mutate(weight = 1 + (1 - row_number())*(1/k))
sorted <- arrange(as.data.frame(cbind(dist, user_id)), desc(dist)) %>% filter(!is.na(dist))
# Weight neighbors from 1 to 0.2
if (length(sorted$dist) == 0) {
avg <- filtered %>% filter(user_id != user)  %>%
summarise(predict = mean(as.numeric(norm_stars)))
avg %>% mutate(user_id = user, business_id = business)
} else {
#Assign active user the average rating of K-NN
avg <- filtered %>% filter(user_id != user) %>%
inner_join(sorted, by ="user_id")  %>%
summarise(predict = sum(as.numeric(dist)*as.numeric(norm_stars))/sum(as.numeric(dist)))
avg %>% mutate(user_id = user, business_id = business)
#avg <- filtered %>% filter(user_id != user) %>%
#  inner_join(sorted, by ="user_id")  %>%
#  summarise(predict = sum(as.numeric(weight)*as.numeric(norm_stars))/sum(as.numeric(weight)))
#avg %>% mutate(user_id = user, business_id = business)
}
}
# For testing
business <- "EH7sT3dV-B365RETWCQhIA"
user <- "nmTG128TupL0-tnhe1h7CQ"
out <- filter_kNN(user, business, 1)
set.seed(203)
# Draw balanced sample to test predictions
positive_sample <- sample(which(test_norm_data$positive == "TRUE"), size = 750, replace = F)
negative_sample <- sample(which(test_norm_data$positive == "FALSE"), size = 750, replace = F)
test <- test_norm_data[c(positive_sample, negative_sample),]
test <- test %>%
select(user_id, business_id, stars, norm_stars, user_avg, user_sd, positive)
predictions  <-  data.frame(user_id = character(), business_id = character(), predict = numeric())
predictions2  <-  data.frame(user_id = character(), business_id = character(), predict = numeric())
# Turn off warnings temporarily
oldw <- getOption("warn")
options(warn = -1)
# Run kNN and save in predictions dataframe
for (i in 1:length(test$user_id)) {
predictions <- rbind(predictions, filter_kNN(as.character(test[i,1]), as.character(test[i,2]), 5))
}
check <- test
check$predict <- predictions$predict
# Calculate accuracy statistics
check <- check %>%
mutate(correct_norm = (positive == (predict > 0)),
predict_unnorm = (predict*user_sd + user_avg),
stars_bin = (stars >= 4),
correct_4 = (stars_bin == (predict_unnorm >= 4)),
sq_diff = ((stars - predict_unnorm)^2),
diff = (stars - predict_unnorm),
abs_diff = ((abs(stars - predict_unnorm))),
user_predict = ((user_avg >= 4) == stars_bin),
user_sq_diff = (user_avg - stars)^2,
user_abs_diff = (abs((user_avg - stars)))
)
# All data in training dataset has been included in the topic models, so test data will be much better measure of success
# Test data can be 2016 reviews from users and restaurants in the training set
(baseline_norm <- sum(check$positive) / length(check$correct_norm))
(accuracy_norm <- sum(check$correct_norm) / length(check$correct_norm))
(baseline_norm <- sum(check2$positive) / length(check2$correct_norm))
(accuracy_norm <- sum(check2$correct_norm) / length(check2$correct_norm))
#(accuracy_4 <- sum(check$correct_4) / length(check$correct_4))
#(baseline_4 <- sum(check$stars_bin) / length(check$correct_norm))
(positive_acc <- sum(check$correct_norm & check$positive) / sum(check$positive == TRUE))
(negative_acc <- sum(check$correct_norm & check$positive == FALSE) / sum(check$positive == FALSE))
(mean(check2$sq_diff))
(mean(check$user_sq_diff))
(mean(check2$abs_diff))
(mean(check$user_abs_diff))
check_analysis <- check %>% inner_join(user_stats, by = "user_id")
check_analysis$cnt_bins <- ceiling(check_analysis$user_count / 5)
ggplot(data = check_analysis) + geom_bar(mapping = aes(x = factor(stars),  fill = correct_norm), position = "fill") +
ylab("Accuracy") + xlab("Actual Rating") + guides(fill = guide_legend(title = "Prediction"))
ggplot(data = check_analysis) + geom_bar(mapping = aes(x = factor(stars))) +
ylab("Count") + xlab("Actual Rating")
ggplot(data = check_analysis) + geom_point(mapping = aes(x = diff, y= stars, alpha = .05, color = correct_norm)) +
ylab("Actual Rating") + xlab("Actual - Predicted ")
ggplot(data = check_analysis, aes(user_count, fill = correct_norm, position = "fill")) +
geom_histogram(breaks=seq(0, 125, by =5)) + ylab("Count") + xlab("Prior User Reviews") +
guides(fill = guide_legend(title = "Prediction"))
ggplot(data = check_analysis) + geom_bar(mapping = aes(x = factor(cnt_bins),  fill = correct_norm), position = "fill")
ggplot(data = check_analysis) + geom_histogram(mapping = aes(x = diff)) + ylab("Count") + xlab("Actual - Predicted")
check_ttest <- check %>% select(sq_diff)
check_ttest$sq_diff_baseline <- check2$sq_diff
t.test(check_ttest$sq_diff, check_ttest$sq_diff_baseline, paired=TRUE)
p <- (0.6153333*1500 + 0.6273333*1500)/3000
SE <- ((p*(1-p))*(1/1500 + 1/1500))^0.5
z <- (0.6153333 - 0.6273333) / SE
(p_value <- dnorm(z))
options(warn = oldw)
View(norm_data)
norm_data
norm_data  %>% group_by(user_id)  %>% count()
25330 / 4003
# Aggregate review text by restaurant id
reviews_by_rest <- aggregate(text ~ business_id, paste, data = norm_data)
# Clean up text for creating restaurant document term matrix
docs_rest <- Corpus(VectorSource(reviews_by_rest$text))
docs_rest <-tm_map(docs_rest,content_transformer(tolower))
#remove punctuation
docs_rest <- tm_map(docs_rest, removePunctuation)
#Strip digits
docs_rest <- tm_map(docs_rest, removeNumbers)
#remove stopwords
docs_rest <- tm_map(docs_rest, removeWords, stopwords("english"))
#remove whitespace
docs_rest <- tm_map(docs_rest, stripWhitespace)
#Create document-term matrix (Limit to terms appearing in at least 10 restaurants and no more than 200 restaurants)
dtm_r <- DocumentTermMatrix(docs_rest, control=list(wordLengths=c(3, 20),
bounds = list(global = c(10,200))))
dtm_merge <- as.data.frame(as.matrix(dtm_r))
dtm_merge$business_id <- reviews_by_rest$business_id
#Set parameters for Gibbs sampling
burnin <- 200
iter <- 300
thin <- 300
seed <-list(2003)
nstart <- 1
best <- TRUE
#Number of topics
k <- 20
ldaOut <-LDA(dtm_r, k, method='Gibbs', control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
library(topicmodels)
ldaOut <-LDA(dtm_r, k, method='Gibbs', control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
ldaOut.topics_r <- as.data.frame(as.matrix(topics(ldaOut)))
ldaOut.terms_r <- as.matrix(terms(ldaOut,10))
View(ldaOut.terms_r)
write.table(ldaOut.terms_r, "~/Documents/INLS_613/top_words20.txt", sep="\t")
install.packages("itsmr")
library(itsmr)
1+0+1+4+9+16
31*(1/6)
16**2
1+0+1+16+81+256
355/6 - (31/6)**2
16/9
library(jsonlite)
review_data <- stream_in(file("~/Documents/COMP_790/yelp_boston_academic_dataset/yelp_academic_dataset_review.json"))
review_flat <- flatten(review_data)
review_tbl <- as_data_frame(review_flat)
View(review_flat)
x  <- c(2, 1.4, 4, 3, 2.2)
x
sd(x)
dif  <- c(.6**2, 1.2**2, 1.4**2, .4**2)
sum(dif)
sum(dif)/4
sum(dif)/3
(sum(dif)/4)**.5
?sd
mean(dif)
mean(x)
sd(x)
set.seed(1)
series1 <- specify(0,0,4)
library(itsmr)
series1 <- specify(0,0,4)
sim(series1, 200)
plot(series1)
plota(series1)
series2 <- specify(0,0.5,3)
sim2 <- sim(series2, 200)
plota(series2)
.5/1.25
(1/16)*(4*5.26 + 2*3*4.74 + 2*2*4.26 + 2*3.84)
(1/16)*(4*5.26 + 2*3*-4.74 + 2*2*4.26 + 2*-3.84)
library(itsmr)
set.seed(1)
series1 <- specify(0,0,4)
sim1 <- sim(series1, 200)
plot(sim1)
plota(sim1)
plot.ts(sim1)
getwd()
knitr::opts_chunk$set(echo = TRUE)
deaths <- scan("deaths.txt")
library(itsmr)
plotc(deaths)
acf(deaths,lag.max=40,type=“correlation”,plot=TRUE)
acf(deaths,lag.max=40,type="correlation",plot=TRUE)
s <- season(deaths,12)
s
?season
plotc(deaths, s)
e <- deaths - s
plotc(e)
acf(e,lag.max=40,type="correlation",plot=TRUE)
?Resid
?trend
s <- season(deaths, 12)
y <- deaths - s
m <- trend(y, 2)
e <- deaths - s - m
plotc(e)
acf(e,lag.max=40,type="correlation",plot=TRUE)
e2 <- deaths - s - m
test(e2)
xv=c("season",12)
a = arma(e2, 0, 0)
forecast(deaths,xv,a,h=24,opt=2)
?arma
a
?forecast
4 + 1.5^2 + .25*4 + 4 + 1.5^2*4 + .25*4
1 + 1.5^2 + 0.25 + 1 + 1.5^2 + 0.25
-1.5 - 1.5 - 0.75 -0.75 -1.5 - 1.5 -0.75 -0.75
(1.5 + 0.75 + 1.5 + 0.75)
4.5*4
1.5^2
-1 - 2.25 - 0.25 - 1 -2.25 - 0.25
4*4.5
knitr::opts_chunk$set(echo = TRUE)
library(itmsr)
library(itsmr)
a=specify(ar=c(0.5),ma=c(0.5),sigma2=1)
?specify
a
a$phi
ar.inf(a,n=5)
?ar.inf
ma.inf(a,n=5)
?pnorm
pnorm(112, 115, (10/(20^0.5)))
10/(20^0.5)
-3/2.236
20^0.5
pnorm(-1.34)
pnorm(1.34)
?qt
qt(.05,24)
qt(.95,24)
5*1.71
6/1.71
3.51*5
1.96*8.97
1.65*.897
20^0.5
-3/(10/4.47)
qt(.05,24)
6/1.71
3.5*5
library(itsmr)
library(itsmr)
library(lda)
library(tidyverse)
library(stringr)
library(tm)
library(glmnet)
library(slam)
setwd("Documents/COMP_790/health_yelp")
raw <- read.csv('data/analysis_data.csv')
raw$pos_text_temp <- trimws(as.character(raw$pos_text))
raw$neg_text_temp <- trimws(as.character(raw$neg_text))
analysis <- raw %>%
mutate(year = str_sub(date, 1, 4),
wgt_viol = (X.*1 + X..*2 + X...*5),
pos_text = ifelse(nchar(pos_text_temp) == 0, 'blank', pos_text_temp),
neg_text = ifelse(nchar(neg_text_temp) == 0, 'blank', neg_text_temp),
all_text = ifelse(nchar(trimws(paste(pos_text_temp, neg_text_temp, set = ""))) == 0, 'blank', paste(pos_text_temp, neg_text_temp)))
analysis %>% group_by(year) %>% summarise(count = n())
train <- analysis %>%
filter(year < 2010) %>%
mutate(log_wgt_viol = log(wgt_viol + 1))
summary(train$wgt_viol)
ggplot(data = train) + geom_histogram(mapping = aes(x = log_wgt_viol))
ggplot(data = train) + geom_histogram(mapping = aes(x = wgt_viol))
mean_baseline_err = mean((train$wgt_viol - 10.44)^2)^0.5
log_mean_baseline_err = mean((train$log_wgt_viol - 1.97)^2)^0.5
View(train)
analysis <- raw %>%
mutate(year = str_sub(date, 1, 4),
wgt_viol = (X.*1 + X..*2 + X...*5),
pos_text = ifelse(nchar(pos_text_temp) == 0, 'blank', pos_text_temp),
neg_text = ifelse(nchar(neg_text_temp) == 0, 'blank', neg_text_temp),
all_text = ifelse(nchar(trimws(paste(pos_text_temp, neg_text_temp, set = ""))) == 0, 'blank', paste(pos_text_temp, neg_text_temp)),
unhygienic <- ifelse(wgt_viol >= 10, 1, 0))
train <- analysis %>%
filter(year < 2010) %>%
mutate(log_wgt_viol = log(wgt_viol + 1))
summary(train$wgt_viol)
analysis <- raw %>%
mutate(year = str_sub(date, 1, 4),
wgt_viol = (X.*1 + X..*2 + X...*5),
pos_text = ifelse(nchar(pos_text_temp) == 0, 'blank', pos_text_temp),
neg_text = ifelse(nchar(neg_text_temp) == 0, 'blank', neg_text_temp),
all_text = ifelse(nchar(trimws(paste(pos_text_temp, neg_text_temp, set = ""))) == 0, 'blank', paste(pos_text_temp, neg_text_temp)),
unhygienic = ifelse(wgt_viol >= 10, 1, 0))
train <- analysis %>%
filter(year < 2010) %>%
mutate(log_wgt_viol = log(wgt_viol + 1))
summary(train$unhygienic)
ggplot(data = train) + geom_histogram(mapping = aes(x = unhygienic))
?glmnet
analysis <- raw %>%
mutate(year = str_sub(date, 1, 4),
wgt_viol = (X.*1 + X..*2 + X...*5),
pos_text = ifelse(nchar(pos_text_temp) == 0, 'blank', pos_text_temp),
neg_text = ifelse(nchar(neg_text_temp) == 0, 'blank', neg_text_temp),
all_text = ifelse(nchar(trimws(paste(pos_text_temp, neg_text_temp, set = ""))) == 0, 'blank', paste(pos_text_temp, neg_text_temp)),
unhygienic = ifelse(wgt_viol >= 7, 1, 0))
train <- analysis %>%
filter(year < 2010) %>%
mutate(log_wgt_viol = log(wgt_viol + 1))
summary(train$unhygienic)
set.seed(10)
sample_train <- train[sample(nrow(train), 1000),]
sample_train <- train[sample(nrow(train), 3000),]
docs <- Corpus(VectorSource(sample_train$all_text))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
dtm <- DocumentTermMatrix(docs, control=list(wordLengths=c(3, 20)))
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm_filter <- dtm[,term_tfidf >= 0.1]
dtm_filter[1]
dtm_filter[1][1]
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing=TRUE)
term_cnts <- col_sums(dtm)
term_cnts[1:5]
term_cnts <- col_sums(v)
v[1:10]
v[1:20]
v[1000:1020]
v[5000:5020]
dtm_filter <- dtm[, term_cnts > 50]
dtm_filter <- dtm[, term_cnts > 100]
dtm_merge <- as.matrix(dtm_filter)
fit <- glmnet(dtm_merge, train$unhygienic, family = "binomial")
fit <- glmnet(dtm_merge, sample_train$unhygienic, family = "binomial")
summary(fit)
fit
coef(fit, s=3.876e-02)
nonzeroCoef(fit)
nonzero(fit)
?nonzeroCoef
?coef
coef.glmnet(fit, s=3.876e-02, "nonzero")
coef.glmnet(fit, s=3.876e-02)
coeffs <- coef.glmnet(fit, s=3.876e-02)
coeffs[1]
coeffs[3]
plot(fit, xvar = "dev", label = TRUE)
set.seed(10)
sample_train <- train[sample(nrow(train), 1000),]
# Clean up text for creating restaurant document term matrix
docs <- Corpus(VectorSource(sample_train$all_text))
docs <-tm_map(docs,content_transformer(tolower))
#remove punctuation
docs <- tm_map(docs, removePunctuation)
#Strip digits
docs <- tm_map(docs, removeNumbers)
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
#Create document-term matrix (Limit to terms appearing in at least 10 restaurants and no more than 200 restaurants)
dtm <- DocumentTermMatrix(docs, control=list(wordLengths=c(3, 20)))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing=TRUE)
term_cnts <- col_sums(v)
dtm_filter <- dtm[, term_cnts > 200]
dtm_filter <- dtm[, term_cnts > 200]
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing=TRUE)
term_cnts <- col_sums(v)
term_cnts <- colSums(v)
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm_filter <- dtm[,term_tfidf >= 0.1]
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing=TRUE)
term_cnts <- colSums(v)
dtm_filter <- dtm[, term_cnts > 200]
dtm_merge <- as.matrix(dtm_filter)
fit <- glmnet(dtm_merge, sample_train$unhygienic, family = "binomial")
plot(fit, xvar = "dev", label = TRUE)
docs[1]
dtm[1]
m[1]
m[1,1]
dtm <- DocumentTermMatrix(docs, control=list(wordLengths=c(3, 20)))
summary(col_sums(dtm))
term_cnts <- col_sums(dtm)
dtm_filter <- dtm[, term_cnts > 200]
dtm_filter <- dtm[, term_cnts > 100]
dtm_filter <- dtm[, term_cnts > 50]
dtm_merge <- as.matrix(dtm_filter)
fit <- glmnet(dtm_merge, sample_train$unhygienic, family = "binomial")
plot(fit, xvar = "dev", label = TRUE)
cvfit <- cv.glmnet(dtm_merge, sample_train$unhygienic, family = "binomial")
plot(cvfit)
str(train$unhygienic)
fit <- glmnet(dtm_merge, factor(sample_train$unhygienic), family = "binomial")
cvfit <- cv.glmnet(dtm_merge, factor(sample_train$unhygienic), family = "binomial")
plot(cvfit)
cvfit <- cv.glmnet(dtm_merge, factor(sample_train$unhygienic), family = "binomial", type.measure = "class")
plot(cvfit)
cvfit$lambda.min
(coeffs <- as.matrix(coef(cvfit, s = "lambda.min")))
sample_train <- train[sample(nrow(train), 3000),]
# Clean up text for creating restaurant document term matrix
docs <- Corpus(VectorSource(sample_train$all_text))
docs <-tm_map(docs,content_transformer(tolower))
#remove punctuation
docs <- tm_map(docs, removePunctuation)
#Strip digits
docs <- tm_map(docs, removeNumbers)
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
#Create document-term matrix (Limit to terms appearing in at least 10 restaurants and no more than 200 restaurants)
dtm <- DocumentTermMatrix(docs, control=list(wordLengths=c(3, 20)))
term_cnts <- col_sums(dtm)
dtm_filter <- dtm[, term_cnts > 50]
dtm_merge <- as.matrix(dtm_filter)
cvfit <- cv.glmnet(dtm_merge, factor(sample_train$unhygienic), family = "binomial", type.measure = "class")
plot(cvfit)
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm_filter <- dtm[,term_tfidf >= 0.1]
dtm <- DocumentTermMatrix(docs, control=list(wordLengths=c(3, 20)))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
dtm_filter <- dtm[,term_tfidf >= 0.1]
dtm_merge <- as.matrix(dtm_filter)
cvfit <- cv.glmnet(dtm_merge, factor(sample_train$unhygienic), family = "binomial", type.measure = "class")
plot(cvfit)
?stemDocument
sample_train <- train[sample(nrow(train), 3000),]
docs <- Corpus(VectorSource(sample_train$all_text))
docs <-tm_map(docs,content_transformer(tolower))
docs <- stemDocument(docs)
docs <- stemDocument(sample_train$all_text)
docs <- Corpus(VectorSource(stemDocument(sample_train$all_text)))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
dtm <- DocumentTermMatrix(docs, control=list(wordLengths=c(3, 20)))
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm_filter <- dtm[,term_tfidf >= 0.1]
dtm_merge <- as.matrix(dtm_filter)
cvfit <- cv.glmnet(dtm_merge, factor(sample_train$unhygienic), family = "binomial", type.measure = "class")
plot(cvfit)
(coeffs <- as.matrix(coef(cvfit, s = "lambda.min")))
removeSparseTerms(dtm, .95)
summary(col_sums(dtm))
dtm2 <- removeSparseTerms(dtm, .95)
dtm2 <- removeSparseTerms(dtm, .975)
cvfit <- cv.glmnet(dtm_merge, factor(sample_train$unhygienic), family = "binomial", type.measure = "class")
docs <- Corpus(VectorSource(sample_train$all_text))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[30]]))
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[30]]))
dtm <- DocumentTermMatrix(docs, control=list(wordLengths=c(3, 20)))
dtm2 <- removeSparseTerms(dtm, .975)
dtm_merge <- as.matrix(dtm2)
cvfit <- cv.glmnet(dtm_merge, factor(sample_train$unhygienic), family = "binomial", type.measure = "class")
plot(cvfit)
cvfit$lambda.min
(coeffs <- as.matrix(coef(cvfit, s = "lambda.min")))
