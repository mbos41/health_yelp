View(dtm_rf)
dtm_rf <- cbind(as.data.frame(as.matrix(dtm)), response)
dtm_rf$response[1:5]
rf_fit <- randomForest(response~, data = dtm_rf)
rf_fit <- randomForest(response~., data = dtm_rf)
rf_fit <- randomForest(response~., data = dtm_rf, importance = TRUE)
rf_fit
varImpPlot(rf_fit)
lasso_terms
imp_terms <- importance(rf_fit, type = 1)
View(imp_terms)
imp_terms <- sort(importance(rf_fit, type = 1), decreasing = TRUE)
imp_terms[1:20]
imp_terms <- importance(rf_fit, type = 1)
View(imp_terms)
View(imp_terms)
rf_terms <- imp_terms[order(-imp_terms$%IncMSE), , drop = FALSE]
rf_terms <- imp_terms[order(-imp_terms$'%IncMSE'), , drop = FALSE]
rf_terms <- imp_terms[order(-imp_terms[,1]), , drop = FALSE]
View(rf_terms)
rf_terms <- imp_terms[order(-imp_terms[,1]), , drop = FALSE][1:20]
rf_terms <- row.names(imp_terms[order(-imp_terms[,1]), , drop = FALSE])
rf_terms <- row.names(imp_terms[order(-imp_terms[,1]), , drop = FALSE])[1:20]
rf_terms
vocab <- rf_terms
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
docs <- lapply(doc.list, get.terms)
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
View(top_words)
View(topic_coeff)
vocab <- lasso_terms
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
summary(col_sums(dtm))
summary(col.sums(dtm))
library(slam)
summary(col.sums(dtm))
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm_filter <- dtm[,term_tfidf > 0.01]
dtm_filter[[1]]
dtm_filter[[2]]
dtm_filter[[2]][1]
dtm_filter
dtm_filter$terms
dtm_filter$dimnames
dtm_filter <- dtm[,term_tfidf > 0.001]
dtm_filter$dimnames
dtm_filter <- dtm[,term_tfidf > 0.005]
dtm_filter$dimnames
noise <- paste0("noise", seq(20))
docLength <- 20
generateDoc <- function(docLength, topic_dist, terms_topics_dist){
document <- c()
topics <- c()
for (i in seq(docLength)){
topic <- rmultinom(1,1, topic_dist)
topics <- c(topics, which.max(topic))
term <- rmultinom(1,1,terms_topics_dist[which.max(topic),])
document <- paste(document, Terms[which.max(term)])
}
zBar <- c()
for (i in seq(length(eta))){
zBar <- c(zBar, mean(topics == i))
}
mu <- t(eta) %*% zBar
y <- rnorm(1, mu, sigma2)
noise_sample <- c()
for (j in seq(200)){noise_sample <- paste(noise_sample, sample(noise, 1, replace= TRUE))}
return(c(y, paste(document, noise_sample)))
}
corpus <- list()
response <-  c()
text <- c()
for (i in seq(M)){
corpus[[i]] <- generateDoc(10, Theta[i,], zn)
response <- rbind(response, as.numeric(corpus[[i]][1]))
text <- rbind(text, (corpus[[i]][2]))
}
docs_dtm <- Corpus(VectorSource(text))
dtm <- DocumentTermMatrix(docs_dtm, control=list(wordLengths=c(3, 30)))
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm_filter <- dtm[,term_tfidf > 0.005]
dtm_filter$dimnames
tfidf_terms <- dtm_filter$dimnames[[2]]
voacb <- tfidf_terms
rm(voacb)
vocab <- tfidf_terms
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
dtm_rf <- cbind(as.data.frame(as.matrix(dtm)), response)
rf_fit <- randomForest(response~., data = dtm_rf, importance = TRUE)
rf_fit
varImpPlot(rf_fit)
imp_terms <- importance(rf_fit, type = 1)
rf_terms <- row.names(imp_terms[order(-imp_terms[,1]), , drop = FALSE])[1:20]
docs_dtm <- Corpus(VectorSource(text))
dtm <- DocumentTermMatrix(docs_dtm, control=list(wordLengths=c(3, 30)))
dtm_model <- sparseMatrix( i = dtm$i, j=dtm$j, x =dtm$v, dimnames = dtm$dimnames)
cvfit <- cv.glmnet(dtm_model, response, nfolds = 5, family = "gaussian", type.measure = "mse")
plot(cvfit)
coeffs <- as.data.frame(as.matrix(coef(cvfit, s = "lambda.min")))
names(coeffs) <- c("coefficient")
coeffs_nonzero <- coeffs %>% rownames_to_column(var = "term") %>% filter(coefficient != 0 & term != "(Intercept)") %>% select(term)
lasso_terms <-coeffs_nonzero[[1]]
vocab <- c(Terms, noise)
doc.list <- strsplit(text, "[[:space:]]+")
vocab <- c(Terms, noise)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
vocab <- c(Terms)
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
vocab <- lasso_terms
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
vocab <- rf_terms
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
vocab <- tfidf_terms
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
term_tfidf[1:4]
term_tfidf[1:40]
noise <- paste0("noise", seq(2000))
generateDoc <- function(docLength, topic_dist, terms_topics_dist){
document <- c()
topics <- c()
for (i in seq(docLength)){
topic <- rmultinom(1,1, topic_dist)
topics <- c(topics, which.max(topic))
term <- rmultinom(1,1,terms_topics_dist[which.max(topic),])
document <- paste(document, Terms[which.max(term)])
}
zBar <- c()
for (i in seq(length(eta))){
zBar <- c(zBar, mean(topics == i))
}
mu <- t(eta) %*% zBar
y <- rnorm(1, mu, sigma2)
noise_sample <- c()
for (j in seq(200)){noise_sample <- paste(noise_sample, sample(noise, 1, replace= TRUE))}
return(c(y, paste(document, noise_sample)))
}
corpus <- list()
response <-  c()
text <- c()
for (i in seq(M)){
corpus[[i]] <- generateDoc(10, Theta[i,], zn)
response <- rbind(response, as.numeric(corpus[[i]][1]))
text <- rbind(text, (corpus[[i]][2]))
}
docs_dtm <- Corpus(VectorSource(text))
dtm <- DocumentTermMatrix(docs_dtm, control=list(wordLengths=c(3, 30)))
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm_filter <- dtm[,term_tfidf > 0.005]
dtm_filter$dimnames
tfidf_terms <- dtm_filter$dimnames[[2]]
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm_filter <- dtm[,term_tfidf > 0.01]
dtm_filter$dimnames
dtm_filter <- dtm[,term_tfidf > 0.02]
dtm_filter$dimnames
dtm_filter <- dtm[,term_tfidf > 0.03]
dtm_filter$dimnames
dtm_filter <- dtm[,term_tfidf > 0.025]
dtm_filter$dimnames
dtm_model <- sparseMatrix( i = dtm$i, j=dtm$j, x =dtm$v, dimnames = dtm$dimnames)
cvfit <- cv.glmnet(dtm_model, response, nfolds = 5, family = "gaussian", type.measure = "mse")
plot(cvfit)
coeffs <- as.data.frame(as.matrix(coef(cvfit, s = "lambda.min")))
names(coeffs) <- c("coefficient")
coeffs_nonzero <- coeffs %>% rownames_to_column(var = "term") %>% filter(coefficient != 0 & term != "(Intercept)") %>% select(term)
lasso_terms <-coeffs_nonzero[[1]]
dtm_rf <- cbind(as.data.frame(as.matrix(dtm)), response)
rf_fit <- randomForest(response~., data = dtm_rf, importance = TRUE)
rf_fit
varImpPlot(rf_fit)
imp_terms <- importance(rf_fit, type = 1)
rf_terms <- row.names(imp_terms[order(-imp_terms[,1]), , drop = FALSE])[1:20]
tfidf_terms <- dtm_filter$dimnames[[2]]
doc.list <- strsplit(text, "[[:space:]]+")
vocab <- tfidf_terms
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
vocab <- rf_terms
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
vocab <- lasso_terms
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
vocab <- c(Terms)
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
vocab <- c(Terms, noise)
docs <- lapply(doc.list, get.terms)
doc_totals <- sapply(docs, length)
response_valid <- response[doc_totals > 0]
docs_valid <- docs[doc_totals > 0]
set.seed(200)
k = 3
params <- sample(c(-1, 1), k, replace=TRUE)
t1 <- Sys.time()
slda_trained <- slda.em(documents = docs_valid, vocab =  vocab, K = k, num.e.iterations = 20, num.m.iterations = 10, variance = 50,
alpha = 1, eta = 0.1, annotations = response_valid, params, logistic = FALSE, method = "sLDA")
t2 <- Sys.time()
t2 - t1
top_words<- top.topic.words(slda_trained$topics, num.words = 10)
topic_coeff <- as.data.frame(slda_trained$coef)
pred_valid <- slda.predict(documents = docs_valid, topics = slda_trained$topics, model = slda_trained$model,
alpha = 0.5, eta = 0.1)
mse <- mean((response_valid - pred_valid)**2)
library(lda)
library(tidyverse)
library(stringr)
library(tm)
library(glmnet)
library(slam)
library(pROC)
setwd("Documents/COMP_790/health_yelp")
raw <- read.csv('data/analysis_data.csv')
raw$pos_text_temp <- trimws(as.character(raw$pos_text))
raw$neg_text_temp <- trimws(as.character(raw$neg_text))
filter_data <- raw %>% mutate(remove1 = (ifelse((as.character(restaurant_id) == as.character(prev_rest)
& (as.Date(date) - as.Date(prev_date)) < 15), 1, 0)),
remove2 = (ifelse((nchar(pos_text_temp) == 0) & (nchar(neg_text_temp) == 0), 1, 0))) %>%
filter((remove1 == 0) & (remove2 == 0))
analysis <- filter_data %>%
mutate(year = str_sub(date, 1, 4),
wgt_viol = (X.*1 + X..*2 + X...*5),
pos_text = ifelse(nchar(pos_text_temp) == 0, 'blank', pos_text_temp),
neg_text = ifelse(nchar(neg_text_temp) == 0, 'blank', neg_text_temp),
all_text = ifelse(nchar(trimws(paste(pos_text_temp, neg_text_temp, set = ""))) == 0, 'blank', paste(pos_text_temp, neg_text_temp)),
unhygienic = ifelse(wgt_viol >= 20, 1, 0)) %>%
select(date, restaurant_id, pos_text, neg_text, all_text, year, wgt_viol, unhygienic)
analysis %>% group_by(year) %>% summarise(count = n())
test <- analysis %>%
filter(year > 2012)
train_valid <- analysis %>%
filter(year < 2013)
ggplot(data = train_valid) + geom_histogram(mapping = aes(x = unhygienic)) + facet_wrap(~year)
set.seed(10)
samples <- sample(nrow(train_valid), nrow(train_valid)*0.75)
sample_train <- train_valid[samples,]
sample_valid <- train_valid[-samples,]
summary(sample_train$unhygienic)
summary(sample_valid$unhygienic)
# Clean up text for creating restaurant document term matrix
docs <- Corpus(VectorSource(sample_train$all_text))
docs <-tm_map(docs,content_transformer(tolower))
#remove punctuation
docs <- tm_map(docs, removePunctuation)
#Strip digits
docs <- tm_map(docs, removeNumbers)
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
dtm <- DocumentTermMatrix(docs, control=list(wordLengths=c(3, 30), bounds = list(global = c(10,Inf))))
str(dtm)
dtm
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
dtm_filter <- dtm[,term_tfidf > 0.01]
all_term <- dtm$dimnames[[2]]
all_term[1:5]
term_tfidf[1:5]
sorted <- sort(term_tfidf, decreasing = TRUE)
sorted[1:5]
sorted <- sort(term_tfidf, decreasing = TRUE)[1:2000]
sorted <- sort(term_tfidf, decreasing = TRUE)[1:2000]
all_term <- dtm$dimnames[[2]][sorted]
all_term <- dtm$dimnames[[2]]
all_term[c(1,2,3)]
sorted[1:3]
sorted[1:3][1]
sorted[1:3][2]
str(sorted)
names(sorted)
sorted <- names(sort(term_tfidf, decreasing = TRUE)[1:2000])
sorted[1:3]
sorted <- names(as.integer(sort(term_tfidf, decreasing = TRUE)[1:2000]))
sorted <- names(as.numeric(sort(term_tfidf, decreasing = TRUE)[1:2000]))
sorted <- names(sort(term_tfidf, decreasing = TRUE)[1:2000])
all_term[c("1", "2", "3")]
sorted <- as.numeric(names(sort(term_tfidf, decreasing = TRUE)[1:2000]))
all_term <- dtm$dimnames[[2]][sorted]
all_term[1:20]
vocab_tfidf <- dtm$dimnames[[2]][sorted]
dtm_model <- sparseMatrix( i = dtm$i, j=dtm$j, x =dtm$v, dimnames = dtm$dimnames)
cvfit <- cv.glmnet(dtm_model, factor(sample_train$unhygienic), nfolds = 5, family = "binomial", type.measure = "class")
plot(cvfit)
coeffs <- as.data.frame(as.matrix(coef(cvfit, s = "lambda.min")))
names(coeffs) <- c("coefficient")
coeffs_nonzero <- coeffs %>% rownames_to_column(var = "term") %>% filter(coefficient != 0 & term != "(Intercept)") %>% select(term)
lasso_terms <-coeffs_nonzero[[1]]
dtm_rf <- cbind(as.data.frame(as.matrix(dtm)), sample_train$unhygienic)
View(dtm_rf)
str(dtm_rf$`sample_train$unhygienic`)
dtm_rf <- cbind(as.data.frame(as.matrix(dtm)), as.factor(sample_train$unhygienic))
str(dtm_rf$`sample_train$unhygienic`)
dtm_rf <- cbind(as.data.frame(as.matrix(dtm)), sample_train$unhygienic)
str(dtm_rf$`sample_train$unhygienic`)
library(randomForest)
?randomForest
rf_fit <- randomForest(as.factor(`sample_train$unhygienic`)~., data = dtm_rf, importance = TRUE)
?protect
?protect()
dtm_rf <- cbind(as.data.frame(as.matrix(dtm)[,1:5000]), sample_train$unhygienic)
rf_fit <- randomForest(as.factor(`sample_train$unhygienic`)~., data = dtm_rf, importance = TRUE)
